{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWCMGlwW19o4",
        "outputId": "78af66bb-ad7e-4e52-a2b4-638ac1c4de7d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "41/41 [==============================] - 273s 6s/step - loss: 1.3199 - val_loss: 0.9675\n",
            "Epoch 2/20\n",
            "41/41 [==============================] - 265s 6s/step - loss: 0.9262 - val_loss: 0.8944\n",
            "Epoch 3/20\n",
            "41/41 [==============================] - 257s 6s/step - loss: 0.8821 - val_loss: 0.8668\n",
            "Epoch 4/20\n",
            "41/41 [==============================] - 257s 6s/step - loss: 0.8616 - val_loss: 0.8508\n",
            "Epoch 5/20\n",
            "41/41 [==============================] - 258s 6s/step - loss: 0.8453 - val_loss: 0.8331\n",
            "Epoch 6/20\n",
            "41/41 [==============================] - 244s 6s/step - loss: 0.8216 - val_loss: 0.7962\n",
            "Epoch 7/20\n",
            "41/41 [==============================] - 241s 6s/step - loss: 0.7549 - val_loss: 0.6983\n",
            "Epoch 8/20\n",
            "41/41 [==============================] - 242s 6s/step - loss: 0.6678 - val_loss: 0.6215\n",
            "Epoch 9/20\n",
            "41/41 [==============================] - 247s 6s/step - loss: 0.6011 - val_loss: 0.5624\n",
            "Epoch 10/20\n",
            "41/41 [==============================] - 241s 6s/step - loss: 0.5536 - val_loss: 0.5223\n",
            "Epoch 11/20\n",
            "41/41 [==============================] - 248s 6s/step - loss: 0.5206 - val_loss: 0.4970\n",
            "Epoch 12/20\n",
            "41/41 [==============================] - 248s 6s/step - loss: 0.4985 - val_loss: 0.4770\n",
            "Epoch 13/20\n",
            "41/41 [==============================] - 247s 6s/step - loss: 0.4803 - val_loss: 0.4599\n",
            "Epoch 14/20\n",
            "41/41 [==============================] - 248s 6s/step - loss: 0.4663 - val_loss: 0.4473\n",
            "Epoch 15/20\n",
            "41/41 [==============================] - 243s 6s/step - loss: 0.4529 - val_loss: 0.4350\n",
            "Epoch 16/20\n",
            "41/41 [==============================] - 240s 6s/step - loss: 0.4419 - val_loss: 0.4244\n",
            "Epoch 17/20\n",
            "41/41 [==============================] - 246s 6s/step - loss: 0.4317 - val_loss: 0.4142\n",
            "Epoch 18/20\n",
            "41/41 [==============================] - 242s 6s/step - loss: 0.4221 - val_loss: 0.4074\n",
            "Epoch 19/20\n",
            "41/41 [==============================] - 242s 6s/step - loss: 0.4140 - val_loss: 0.4002\n",
            "Epoch 20/20\n",
            "41/41 [==============================] - 250s 6s/step - loss: 0.4065 - val_loss: 0.3914\n",
            "328/328 [==============================] - 42s 128ms/step - loss: 0.3914\n",
            "Accuracy: 39.14%\n",
            "1/1 [==============================] - 1s 866ms/step\n",
            "Generated Word: wagg\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "Generated Word: contentiddl\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "Generated Word: pallddin\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "Generated Word: acuitt\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "Generated Word: hartbrooonn\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('orthodata.csv')\n",
        "\n",
        "words_column = 'words'\n",
        "phonetic_column = 'IPA'\n",
        "\n",
        "# Convert NaN values to empty strings\n",
        "data[words_column] = data[words_column].astype(str)\n",
        "data[phonetic_column] = data[phonetic_column].astype(str)\n",
        "\n",
        "# Extract English words and split them into characters\n",
        "words = data[words_column]\n",
        "phonetic_forms = data[phonetic_column]\n",
        "\n",
        "# Join all characters to build a vocabulary\n",
        "all_characters = sorted(set(' '.join(words) + ' '.join(phonetic_forms)))\n",
        "\n",
        "# Use Keras Tokenizer to encode characters\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(all_characters)\n",
        "\n",
        "# Convert words and phonetic forms to integer sequences\n",
        "X_phonetic_sequences = tokenizer.texts_to_sequences(phonetic_forms)\n",
        "Y_orthographic_sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_sequence_length = max(len(seq) for seq in X_phonetic_sequences + Y_orthographic_sequences)\n",
        "X_phonetic_padded = pad_sequences(X_phonetic_sequences, maxlen=max_sequence_length, padding='post')\n",
        "Y_orthographic_padded = pad_sequences(Y_orthographic_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_phonetic_padded, Y_orthographic_padded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the seq2seq autoencoder model with increased complexity\n",
        "latent_dim = 256  # Increased latent dimension\n",
        "\n",
        "encoder_inputs = Input(shape=(max_sequence_length,))\n",
        "encoder_embedding = Embedding(len(tokenizer.word_index) + 1, latent_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(encoder_embedding)\n",
        "encoder_lstm = LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2)(encoder_lstm)\n",
        "\n",
        "decoder_inputs = Input(shape=(max_sequence_length,))\n",
        "decoder_embedding = Embedding(len(tokenizer.word_index) + 1, latent_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(decoder_embedding)\n",
        "decoder_lstm = LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(decoder_lstm)\n",
        "decoder_dense = TimeDistributed(Dense(len(tokenizer.word_index) + 1, activation='softmax'))(decoder_lstm)\n",
        "autoencoder = Model([encoder_inputs, decoder_inputs], decoder_dense)\n",
        "\n",
        "\n",
        "autoencoder.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit([X_train, X_train], Y_train, epochs=20, batch_size=1024, validation_data=([X_test, X_test], Y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = autoencoder.evaluate([X_test, X_test], Y_test)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Generate five random words using the encoder-decoder model\n",
        "for _ in range(5):\n",
        "    random_input_vector = X_test[np.random.randint(0, X_test.shape[0])]\n",
        "    random_input_vector = np.expand_dims(random_input_vector, axis=0)  # Add batch dimension\n",
        "    decoded_word = autoencoder.predict([random_input_vector, random_input_vector])\n",
        "\n",
        "    # Convert the decoded word to characters\n",
        "    decoded_word_chars = [tokenizer.index_word[i] for i in np.argmax(decoded_word, axis=2)[0] if i != 0]\n",
        "\n",
        "    print('Generated Word:', ''.join(decoded_word_chars))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate five random words using the encoder-decoder model\n",
        "original_words = []\n",
        "converted_words = []\n",
        "\n",
        "for _ in range(5):\n",
        "    random_index = np.random.randint(0, X_test.shape[0])\n",
        "    random_input_vector = X_test[random_index]\n",
        "    random_input_vector = np.expand_dims(random_input_vector, axis=0)  # Add batch dimension\n",
        "    decoded_word = autoencoder.predict([random_input_vector, random_input_vector])\n",
        "\n",
        "    # Convert the decoded word to characters\n",
        "    decoded_word_chars = [tokenizer.index_word[i] for i in np.argmax(decoded_word, axis=2)[0] if i != 0]\n",
        "\n",
        "    original_word = ' '.join([tokenizer.index_word[i] for i in X_test[random_index] if i != 0])\n",
        "    converted_word = ''.join(decoded_word_chars)\n",
        "\n",
        "    original_words.append(original_word)\n",
        "    converted_words.append(converted_word)\n",
        "\n",
        "    print(f'Original Word: {original_word} | Converted Word: {converted_word}')\n",
        "\n",
        "# Print the original and converted words lists\n",
        "print('Original Words:', original_words)\n",
        "print('Converted Words:', converted_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujn7nVDJQnNn",
        "outputId": "b1fef172-e822-494d-dbaa-94eb9e17859d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 73ms/step\n",
            "Original Word: l e i t @ n t | Converted Word: leiernt\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "Original Word: t r o l i b v s | Converted Word: trollbbus\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "Original Word: p l e i n | Converted Word: plain\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "Original Word: p e i n t @ r ɞ | Converted Word: peinter\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "Original Word: l o n h ɑ n d | Converted Word: longandd\n",
            "Original Words: ['l e i t @ n t', 't r o l i b v s', 'p l e i n', 'p e i n t @ r ɞ', 'l o n h ɑ n d']\n",
            "Converted Words: ['leiernt', 'trollbbus', 'plain', 'peinter', 'longandd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WL5gk2zeDv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}